{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "              ReLU-3           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-4           [-1, 64, 16, 16]               0\n",
      "            Conv2d-5           [-1, 64, 16, 16]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 16, 16]             128\n",
      "              ReLU-7           [-1, 64, 16, 16]               0\n",
      "            Conv2d-8           [-1, 64, 16, 16]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 16, 16]             128\n",
      "             ReLU-10           [-1, 64, 16, 16]               0\n",
      "       BasicBlock-11           [-1, 64, 16, 16]               0\n",
      "           Conv2d-12           [-1, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 16, 16]             128\n",
      "             ReLU-14           [-1, 64, 16, 16]               0\n",
      "           Conv2d-15           [-1, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 16, 16]             128\n",
      "             ReLU-17           [-1, 64, 16, 16]               0\n",
      "       BasicBlock-18           [-1, 64, 16, 16]               0\n",
      "           Conv2d-19            [-1, 128, 8, 8]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 8, 8]             256\n",
      "             ReLU-21            [-1, 128, 8, 8]               0\n",
      "           Conv2d-22            [-1, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 8, 8]             256\n",
      "           Conv2d-24            [-1, 128, 8, 8]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 8, 8]             256\n",
      "             ReLU-26            [-1, 128, 8, 8]               0\n",
      "       BasicBlock-27            [-1, 128, 8, 8]               0\n",
      "           Conv2d-28            [-1, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 8, 8]             256\n",
      "             ReLU-30            [-1, 128, 8, 8]               0\n",
      "           Conv2d-31            [-1, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 8, 8]             256\n",
      "             ReLU-33            [-1, 128, 8, 8]               0\n",
      "       BasicBlock-34            [-1, 128, 8, 8]               0\n",
      "           Conv2d-35            [-1, 256, 4, 4]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 4, 4]             512\n",
      "             ReLU-37            [-1, 256, 4, 4]               0\n",
      "           Conv2d-38            [-1, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 4, 4]             512\n",
      "           Conv2d-40            [-1, 256, 4, 4]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 4, 4]             512\n",
      "             ReLU-42            [-1, 256, 4, 4]               0\n",
      "       BasicBlock-43            [-1, 256, 4, 4]               0\n",
      "           Conv2d-44            [-1, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 4, 4]             512\n",
      "             ReLU-46            [-1, 256, 4, 4]               0\n",
      "           Conv2d-47            [-1, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 4, 4]             512\n",
      "             ReLU-49            [-1, 256, 4, 4]               0\n",
      "       BasicBlock-50            [-1, 256, 4, 4]               0\n",
      "           Conv2d-51            [-1, 512, 2, 2]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-53            [-1, 512, 2, 2]               0\n",
      "           Conv2d-54            [-1, 512, 2, 2]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 2, 2]           1,024\n",
      "           Conv2d-56            [-1, 512, 2, 2]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-58            [-1, 512, 2, 2]               0\n",
      "       BasicBlock-59            [-1, 512, 2, 2]               0\n",
      "           Conv2d-60            [-1, 512, 2, 2]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-62            [-1, 512, 2, 2]               0\n",
      "           Conv2d-63            [-1, 512, 2, 2]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-65            [-1, 512, 2, 2]               0\n",
      "       BasicBlock-66            [-1, 512, 2, 2]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 11,176,512\n",
      "Trainable params: 11,176,512\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 5.13\n",
      "Params size (MB): 42.64\n",
      "Estimated Total Size (MB): 47.81\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "model = resnet18(weights=weights, progress=False).to('cuda')\n",
    "layers=list(model.children())\n",
    "model = nn.Sequential(*layers[:-1]).to('cuda')\n",
    "summary(model, (3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/remote-home/xjzhao/anaconda3/envs/countr/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)ip_pytorch_model.bin: 100%|██████████| 10.2G/10.2G [12:28<00:00, 13.6MB/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tmp/datasets/images_384_VarV2/2004.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model, _, preprocess \u001b[39m=\u001b[39m open_clip\u001b[39m.\u001b[39mcreate_model_and_transforms(\u001b[39m'\u001b[39m\u001b[39mViT-bigG-14\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlaion2b_s39b_b160k\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[39m=\u001b[39m open_clip\u001b[39m.\u001b[39mget_tokenizer(\u001b[39m'\u001b[39m\u001b[39mViT-bigG-14\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m image \u001b[39m=\u001b[39m preprocess(Image\u001b[39m.\u001b[39;49mopen(\u001b[39m\"\u001b[39;49m\u001b[39m./tmp/datasets/images_384_VarV2/2004.jpg\u001b[39;49m\u001b[39m\"\u001b[39;49m))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(image\u001b[39m.\u001b[39mshape)  \u001b[39m# prints: torch.Size([1, 3, 224, 224])\u001b[39;00m\n\u001b[1;32m     10\u001b[0m text \u001b[39m=\u001b[39m tokenizer([\u001b[39m\"\u001b[39m\u001b[39msunglasses\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/countr/lib/python3.9/site-packages/PIL/Image.py:3236\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3233\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[1;32m   3235\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m-> 3236\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   3237\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   3239\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tmp/datasets/images_384_VarV2/2004.jpg'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-bigG-14', 'laion2b_s39b_b160k')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-bigG-14')\n",
    "\n",
    "image = preprocess(Image.open(\"./tmp/datasets/images_384_VarV2/2004.jpg\")).unsqueeze(0)\n",
    "print(image.shape)  # prints: torch.Size([1, 3, 224, 224])\n",
    "text = tokenizer([\"sunglasses\"])\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    print(image_features.shape)  # prints: torch.Size([1, 512])\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m x\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mrandn(\u001b[39m8\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m x\u001b[39m=\u001b[39mmodel(x)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/countr/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/countr/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/countr/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/countr/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/countr/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x=torch.randn(8, 3, 64, 64)\n",
    "x=model(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer([\"a diagram\", \"sunglasses\", \"a cat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = model.encode_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─PatchEmbed: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       787,456\n",
      "|    └─Identity: 2-2                     --\n",
      "├─ModuleList: 1-2                        --\n",
      "|    └─Block: 2-3                        --\n",
      "|    |    └─LayerNorm: 3-1               2,048\n",
      "|    |    └─Attention: 3-2               4,198,400\n",
      "|    |    └─Identity: 3-3                --\n",
      "|    |    └─Identity: 3-4                --\n",
      "|    |    └─LayerNorm: 3-5               2,048\n",
      "|    |    └─Mlp: 3-6                     8,393,728\n",
      "|    |    └─Identity: 3-7                --\n",
      "|    |    └─Identity: 3-8                --\n",
      "|    └─Block: 2-4                        --\n",
      "|    |    └─LayerNorm: 3-9               2,048\n",
      "|    |    └─Attention: 3-10              4,198,400\n",
      "|    |    └─Identity: 3-11               --\n",
      "|    |    └─Identity: 3-12               --\n",
      "|    |    └─LayerNorm: 3-13              2,048\n",
      "|    |    └─Mlp: 3-14                    8,393,728\n",
      "|    |    └─Identity: 3-15               --\n",
      "|    |    └─Identity: 3-16               --\n",
      "|    └─Block: 2-5                        --\n",
      "|    |    └─LayerNorm: 3-17              2,048\n",
      "|    |    └─Attention: 3-18              4,198,400\n",
      "|    |    └─Identity: 3-19               --\n",
      "|    |    └─Identity: 3-20               --\n",
      "|    |    └─LayerNorm: 3-21              2,048\n",
      "|    |    └─Mlp: 3-22                    8,393,728\n",
      "|    |    └─Identity: 3-23               --\n",
      "|    |    └─Identity: 3-24               --\n",
      "|    └─Block: 2-6                        --\n",
      "|    |    └─LayerNorm: 3-25              2,048\n",
      "|    |    └─Attention: 3-26              4,198,400\n",
      "|    |    └─Identity: 3-27               --\n",
      "|    |    └─Identity: 3-28               --\n",
      "|    |    └─LayerNorm: 3-29              2,048\n",
      "|    |    └─Mlp: 3-30                    8,393,728\n",
      "|    |    └─Identity: 3-31               --\n",
      "|    |    └─Identity: 3-32               --\n",
      "|    └─Block: 2-7                        --\n",
      "|    |    └─LayerNorm: 3-33              2,048\n",
      "|    |    └─Attention: 3-34              4,198,400\n",
      "|    |    └─Identity: 3-35               --\n",
      "|    |    └─Identity: 3-36               --\n",
      "|    |    └─LayerNorm: 3-37              2,048\n",
      "|    |    └─Mlp: 3-38                    8,393,728\n",
      "|    |    └─Identity: 3-39               --\n",
      "|    |    └─Identity: 3-40               --\n",
      "|    └─Block: 2-8                        --\n",
      "|    |    └─LayerNorm: 3-41              2,048\n",
      "|    |    └─Attention: 3-42              4,198,400\n",
      "|    |    └─Identity: 3-43               --\n",
      "|    |    └─Identity: 3-44               --\n",
      "|    |    └─LayerNorm: 3-45              2,048\n",
      "|    |    └─Mlp: 3-46                    8,393,728\n",
      "|    |    └─Identity: 3-47               --\n",
      "|    |    └─Identity: 3-48               --\n",
      "|    └─Block: 2-9                        --\n",
      "|    |    └─LayerNorm: 3-49              2,048\n",
      "|    |    └─Attention: 3-50              4,198,400\n",
      "|    |    └─Identity: 3-51               --\n",
      "|    |    └─Identity: 3-52               --\n",
      "|    |    └─LayerNorm: 3-53              2,048\n",
      "|    |    └─Mlp: 3-54                    8,393,728\n",
      "|    |    └─Identity: 3-55               --\n",
      "|    |    └─Identity: 3-56               --\n",
      "|    └─Block: 2-10                       --\n",
      "|    |    └─LayerNorm: 3-57              2,048\n",
      "|    |    └─Attention: 3-58              4,198,400\n",
      "|    |    └─Identity: 3-59               --\n",
      "|    |    └─Identity: 3-60               --\n",
      "|    |    └─LayerNorm: 3-61              2,048\n",
      "|    |    └─Mlp: 3-62                    8,393,728\n",
      "|    |    └─Identity: 3-63               --\n",
      "|    |    └─Identity: 3-64               --\n",
      "|    └─Block: 2-11                       --\n",
      "|    |    └─LayerNorm: 3-65              2,048\n",
      "|    |    └─Attention: 3-66              4,198,400\n",
      "|    |    └─Identity: 3-67               --\n",
      "|    |    └─Identity: 3-68               --\n",
      "|    |    └─LayerNorm: 3-69              2,048\n",
      "|    |    └─Mlp: 3-70                    8,393,728\n",
      "|    |    └─Identity: 3-71               --\n",
      "|    |    └─Identity: 3-72               --\n",
      "|    └─Block: 2-12                       --\n",
      "|    |    └─LayerNorm: 3-73              2,048\n",
      "|    |    └─Attention: 3-74              4,198,400\n",
      "|    |    └─Identity: 3-75               --\n",
      "|    |    └─Identity: 3-76               --\n",
      "|    |    └─LayerNorm: 3-77              2,048\n",
      "|    |    └─Mlp: 3-78                    8,393,728\n",
      "|    |    └─Identity: 3-79               --\n",
      "|    |    └─Identity: 3-80               --\n",
      "|    └─Block: 2-13                       --\n",
      "|    |    └─LayerNorm: 3-81              2,048\n",
      "|    |    └─Attention: 3-82              4,198,400\n",
      "|    |    └─Identity: 3-83               --\n",
      "|    |    └─Identity: 3-84               --\n",
      "|    |    └─LayerNorm: 3-85              2,048\n",
      "|    |    └─Mlp: 3-86                    8,393,728\n",
      "|    |    └─Identity: 3-87               --\n",
      "|    |    └─Identity: 3-88               --\n",
      "|    └─Block: 2-14                       --\n",
      "|    |    └─LayerNorm: 3-89              2,048\n",
      "|    |    └─Attention: 3-90              4,198,400\n",
      "|    |    └─Identity: 3-91               --\n",
      "|    |    └─Identity: 3-92               --\n",
      "|    |    └─LayerNorm: 3-93              2,048\n",
      "|    |    └─Mlp: 3-94                    8,393,728\n",
      "|    |    └─Identity: 3-95               --\n",
      "|    |    └─Identity: 3-96               --\n",
      "|    └─Block: 2-15                       --\n",
      "|    |    └─LayerNorm: 3-97              2,048\n",
      "|    |    └─Attention: 3-98              4,198,400\n",
      "|    |    └─Identity: 3-99               --\n",
      "|    |    └─Identity: 3-100              --\n",
      "|    |    └─LayerNorm: 3-101             2,048\n",
      "|    |    └─Mlp: 3-102                   8,393,728\n",
      "|    |    └─Identity: 3-103              --\n",
      "|    |    └─Identity: 3-104              --\n",
      "|    └─Block: 2-16                       --\n",
      "|    |    └─LayerNorm: 3-105             2,048\n",
      "|    |    └─Attention: 3-106             4,198,400\n",
      "|    |    └─Identity: 3-107              --\n",
      "|    |    └─Identity: 3-108              --\n",
      "|    |    └─LayerNorm: 3-109             2,048\n",
      "|    |    └─Mlp: 3-110                   8,393,728\n",
      "|    |    └─Identity: 3-111              --\n",
      "|    |    └─Identity: 3-112              --\n",
      "|    └─Block: 2-17                       --\n",
      "|    |    └─LayerNorm: 3-113             2,048\n",
      "|    |    └─Attention: 3-114             4,198,400\n",
      "|    |    └─Identity: 3-115              --\n",
      "|    |    └─Identity: 3-116              --\n",
      "|    |    └─LayerNorm: 3-117             2,048\n",
      "|    |    └─Mlp: 3-118                   8,393,728\n",
      "|    |    └─Identity: 3-119              --\n",
      "|    |    └─Identity: 3-120              --\n",
      "|    └─Block: 2-18                       --\n",
      "|    |    └─LayerNorm: 3-121             2,048\n",
      "|    |    └─Attention: 3-122             4,198,400\n",
      "|    |    └─Identity: 3-123              --\n",
      "|    |    └─Identity: 3-124              --\n",
      "|    |    └─LayerNorm: 3-125             2,048\n",
      "|    |    └─Mlp: 3-126                   8,393,728\n",
      "|    |    └─Identity: 3-127              --\n",
      "|    |    └─Identity: 3-128              --\n",
      "|    └─Block: 2-19                       --\n",
      "|    |    └─LayerNorm: 3-129             2,048\n",
      "|    |    └─Attention: 3-130             4,198,400\n",
      "|    |    └─Identity: 3-131              --\n",
      "|    |    └─Identity: 3-132              --\n",
      "|    |    └─LayerNorm: 3-133             2,048\n",
      "|    |    └─Mlp: 3-134                   8,393,728\n",
      "|    |    └─Identity: 3-135              --\n",
      "|    |    └─Identity: 3-136              --\n",
      "|    └─Block: 2-20                       --\n",
      "|    |    └─LayerNorm: 3-137             2,048\n",
      "|    |    └─Attention: 3-138             4,198,400\n",
      "|    |    └─Identity: 3-139              --\n",
      "|    |    └─Identity: 3-140              --\n",
      "|    |    └─LayerNorm: 3-141             2,048\n",
      "|    |    └─Mlp: 3-142                   8,393,728\n",
      "|    |    └─Identity: 3-143              --\n",
      "|    |    └─Identity: 3-144              --\n",
      "|    └─Block: 2-21                       --\n",
      "|    |    └─LayerNorm: 3-145             2,048\n",
      "|    |    └─Attention: 3-146             4,198,400\n",
      "|    |    └─Identity: 3-147              --\n",
      "|    |    └─Identity: 3-148              --\n",
      "|    |    └─LayerNorm: 3-149             2,048\n",
      "|    |    └─Mlp: 3-150                   8,393,728\n",
      "|    |    └─Identity: 3-151              --\n",
      "|    |    └─Identity: 3-152              --\n",
      "|    └─Block: 2-22                       --\n",
      "|    |    └─LayerNorm: 3-153             2,048\n",
      "|    |    └─Attention: 3-154             4,198,400\n",
      "|    |    └─Identity: 3-155              --\n",
      "|    |    └─Identity: 3-156              --\n",
      "|    |    └─LayerNorm: 3-157             2,048\n",
      "|    |    └─Mlp: 3-158                   8,393,728\n",
      "|    |    └─Identity: 3-159              --\n",
      "|    |    └─Identity: 3-160              --\n",
      "|    └─Block: 2-23                       --\n",
      "|    |    └─LayerNorm: 3-161             2,048\n",
      "|    |    └─Attention: 3-162             4,198,400\n",
      "|    |    └─Identity: 3-163              --\n",
      "|    |    └─Identity: 3-164              --\n",
      "|    |    └─LayerNorm: 3-165             2,048\n",
      "|    |    └─Mlp: 3-166                   8,393,728\n",
      "|    |    └─Identity: 3-167              --\n",
      "|    |    └─Identity: 3-168              --\n",
      "|    └─Block: 2-24                       --\n",
      "|    |    └─LayerNorm: 3-169             2,048\n",
      "|    |    └─Attention: 3-170             4,198,400\n",
      "|    |    └─Identity: 3-171              --\n",
      "|    |    └─Identity: 3-172              --\n",
      "|    |    └─LayerNorm: 3-173             2,048\n",
      "|    |    └─Mlp: 3-174                   8,393,728\n",
      "|    |    └─Identity: 3-175              --\n",
      "|    |    └─Identity: 3-176              --\n",
      "|    └─Block: 2-25                       --\n",
      "|    |    └─LayerNorm: 3-177             2,048\n",
      "|    |    └─Attention: 3-178             4,198,400\n",
      "|    |    └─Identity: 3-179              --\n",
      "|    |    └─Identity: 3-180              --\n",
      "|    |    └─LayerNorm: 3-181             2,048\n",
      "|    |    └─Mlp: 3-182                   8,393,728\n",
      "|    |    └─Identity: 3-183              --\n",
      "|    |    └─Identity: 3-184              --\n",
      "|    └─Block: 2-26                       --\n",
      "|    |    └─LayerNorm: 3-185             2,048\n",
      "|    |    └─Attention: 3-186             4,198,400\n",
      "|    |    └─Identity: 3-187              --\n",
      "|    |    └─Identity: 3-188              --\n",
      "|    |    └─LayerNorm: 3-189             2,048\n",
      "|    |    └─Mlp: 3-190                   8,393,728\n",
      "|    |    └─Identity: 3-191              --\n",
      "|    |    └─Identity: 3-192              --\n",
      "├─LayerNorm: 1-3                         2,048\n",
      "├─Linear: 1-4                            524,800\n",
      "├─Sequential: 1-5                        --\n",
      "|    └─Conv2d: 2-27                      1,792\n",
      "|    └─InstanceNorm2d: 2-28              --\n",
      "|    └─ReLU: 2-29                        --\n",
      "|    └─MaxPool2d: 2-30                   --\n",
      "├─Sequential: 1-6                        --\n",
      "|    └─Conv2d: 2-31                      73,856\n",
      "|    └─InstanceNorm2d: 2-32              --\n",
      "|    └─ReLU: 2-33                        --\n",
      "|    └─MaxPool2d: 2-34                   --\n",
      "├─Sequential: 1-7                        --\n",
      "|    └─Conv2d: 2-35                      295,168\n",
      "|    └─InstanceNorm2d: 2-36              --\n",
      "|    └─ReLU: 2-37                        --\n",
      "|    └─MaxPool2d: 2-38                   --\n",
      "├─Sequential: 1-8                        --\n",
      "|    └─Conv2d: 2-39                      1,180,160\n",
      "|    └─InstanceNorm2d: 2-40              --\n",
      "|    └─ReLU: 2-41                        --\n",
      "|    └─AdaptiveAvgPool2d: 2-42           --\n",
      "├─ModuleList: 1-9                        --\n",
      "|    └─CrossAttentionBlock: 2-43         --\n",
      "|    |    └─LayerNorm: 3-193             1,024\n",
      "|    |    └─Attention: 3-194             1,050,624\n",
      "|    |    └─Identity: 3-195              --\n",
      "|    |    └─LayerNorm: 3-196             1,024\n",
      "|    |    └─CrossAttention: 3-197        1,050,624\n",
      "|    |    └─Identity: 3-198              --\n",
      "|    |    └─LayerNorm: 3-199             1,024\n",
      "|    |    └─Mlp: 3-200                   2,099,712\n",
      "|    |    └─Identity: 3-201              --\n",
      "|    └─CrossAttentionBlock: 2-44         --\n",
      "|    |    └─LayerNorm: 3-202             1,024\n",
      "|    |    └─Attention: 3-203             1,050,624\n",
      "|    |    └─Identity: 3-204              --\n",
      "|    |    └─LayerNorm: 3-205             1,024\n",
      "|    |    └─CrossAttention: 3-206        1,050,624\n",
      "|    |    └─Identity: 3-207              --\n",
      "|    |    └─LayerNorm: 3-208             1,024\n",
      "|    |    └─Mlp: 3-209                   2,099,712\n",
      "|    |    └─Identity: 3-210              --\n",
      "├─LayerNorm: 1-10                        1,024\n",
      "├─Sequential: 1-11                       --\n",
      "|    └─Conv2d: 2-45                      1,179,904\n",
      "|    └─GroupNorm: 2-46                   512\n",
      "|    └─ReLU: 2-47                        --\n",
      "├─Sequential: 1-12                       --\n",
      "|    └─Conv2d: 2-48                      590,080\n",
      "|    └─GroupNorm: 2-49                   512\n",
      "|    └─ReLU: 2-50                        --\n",
      "├─Sequential: 1-13                       --\n",
      "|    └─Conv2d: 2-51                      590,080\n",
      "|    └─GroupNorm: 2-52                   512\n",
      "|    └─ReLU: 2-53                        --\n",
      "├─Sequential: 1-14                       --\n",
      "|    └─Conv2d: 2-54                      590,080\n",
      "|    └─GroupNorm: 2-55                   512\n",
      "|    └─ReLU: 2-56                        --\n",
      "|    └─Conv2d: 2-57                      257\n",
      "=================================================================\n",
      "Total params: 316,536,193\n",
      "Trainable params: 316,536,193\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─PatchEmbed: 1-1                        --\n",
       "|    └─Conv2d: 2-1                       787,456\n",
       "|    └─Identity: 2-2                     --\n",
       "├─ModuleList: 1-2                        --\n",
       "|    └─Block: 2-3                        --\n",
       "|    |    └─LayerNorm: 3-1               2,048\n",
       "|    |    └─Attention: 3-2               4,198,400\n",
       "|    |    └─Identity: 3-3                --\n",
       "|    |    └─Identity: 3-4                --\n",
       "|    |    └─LayerNorm: 3-5               2,048\n",
       "|    |    └─Mlp: 3-6                     8,393,728\n",
       "|    |    └─Identity: 3-7                --\n",
       "|    |    └─Identity: 3-8                --\n",
       "|    └─Block: 2-4                        --\n",
       "|    |    └─LayerNorm: 3-9               2,048\n",
       "|    |    └─Attention: 3-10              4,198,400\n",
       "|    |    └─Identity: 3-11               --\n",
       "|    |    └─Identity: 3-12               --\n",
       "|    |    └─LayerNorm: 3-13              2,048\n",
       "|    |    └─Mlp: 3-14                    8,393,728\n",
       "|    |    └─Identity: 3-15               --\n",
       "|    |    └─Identity: 3-16               --\n",
       "|    └─Block: 2-5                        --\n",
       "|    |    └─LayerNorm: 3-17              2,048\n",
       "|    |    └─Attention: 3-18              4,198,400\n",
       "|    |    └─Identity: 3-19               --\n",
       "|    |    └─Identity: 3-20               --\n",
       "|    |    └─LayerNorm: 3-21              2,048\n",
       "|    |    └─Mlp: 3-22                    8,393,728\n",
       "|    |    └─Identity: 3-23               --\n",
       "|    |    └─Identity: 3-24               --\n",
       "|    └─Block: 2-6                        --\n",
       "|    |    └─LayerNorm: 3-25              2,048\n",
       "|    |    └─Attention: 3-26              4,198,400\n",
       "|    |    └─Identity: 3-27               --\n",
       "|    |    └─Identity: 3-28               --\n",
       "|    |    └─LayerNorm: 3-29              2,048\n",
       "|    |    └─Mlp: 3-30                    8,393,728\n",
       "|    |    └─Identity: 3-31               --\n",
       "|    |    └─Identity: 3-32               --\n",
       "|    └─Block: 2-7                        --\n",
       "|    |    └─LayerNorm: 3-33              2,048\n",
       "|    |    └─Attention: 3-34              4,198,400\n",
       "|    |    └─Identity: 3-35               --\n",
       "|    |    └─Identity: 3-36               --\n",
       "|    |    └─LayerNorm: 3-37              2,048\n",
       "|    |    └─Mlp: 3-38                    8,393,728\n",
       "|    |    └─Identity: 3-39               --\n",
       "|    |    └─Identity: 3-40               --\n",
       "|    └─Block: 2-8                        --\n",
       "|    |    └─LayerNorm: 3-41              2,048\n",
       "|    |    └─Attention: 3-42              4,198,400\n",
       "|    |    └─Identity: 3-43               --\n",
       "|    |    └─Identity: 3-44               --\n",
       "|    |    └─LayerNorm: 3-45              2,048\n",
       "|    |    └─Mlp: 3-46                    8,393,728\n",
       "|    |    └─Identity: 3-47               --\n",
       "|    |    └─Identity: 3-48               --\n",
       "|    └─Block: 2-9                        --\n",
       "|    |    └─LayerNorm: 3-49              2,048\n",
       "|    |    └─Attention: 3-50              4,198,400\n",
       "|    |    └─Identity: 3-51               --\n",
       "|    |    └─Identity: 3-52               --\n",
       "|    |    └─LayerNorm: 3-53              2,048\n",
       "|    |    └─Mlp: 3-54                    8,393,728\n",
       "|    |    └─Identity: 3-55               --\n",
       "|    |    └─Identity: 3-56               --\n",
       "|    └─Block: 2-10                       --\n",
       "|    |    └─LayerNorm: 3-57              2,048\n",
       "|    |    └─Attention: 3-58              4,198,400\n",
       "|    |    └─Identity: 3-59               --\n",
       "|    |    └─Identity: 3-60               --\n",
       "|    |    └─LayerNorm: 3-61              2,048\n",
       "|    |    └─Mlp: 3-62                    8,393,728\n",
       "|    |    └─Identity: 3-63               --\n",
       "|    |    └─Identity: 3-64               --\n",
       "|    └─Block: 2-11                       --\n",
       "|    |    └─LayerNorm: 3-65              2,048\n",
       "|    |    └─Attention: 3-66              4,198,400\n",
       "|    |    └─Identity: 3-67               --\n",
       "|    |    └─Identity: 3-68               --\n",
       "|    |    └─LayerNorm: 3-69              2,048\n",
       "|    |    └─Mlp: 3-70                    8,393,728\n",
       "|    |    └─Identity: 3-71               --\n",
       "|    |    └─Identity: 3-72               --\n",
       "|    └─Block: 2-12                       --\n",
       "|    |    └─LayerNorm: 3-73              2,048\n",
       "|    |    └─Attention: 3-74              4,198,400\n",
       "|    |    └─Identity: 3-75               --\n",
       "|    |    └─Identity: 3-76               --\n",
       "|    |    └─LayerNorm: 3-77              2,048\n",
       "|    |    └─Mlp: 3-78                    8,393,728\n",
       "|    |    └─Identity: 3-79               --\n",
       "|    |    └─Identity: 3-80               --\n",
       "|    └─Block: 2-13                       --\n",
       "|    |    └─LayerNorm: 3-81              2,048\n",
       "|    |    └─Attention: 3-82              4,198,400\n",
       "|    |    └─Identity: 3-83               --\n",
       "|    |    └─Identity: 3-84               --\n",
       "|    |    └─LayerNorm: 3-85              2,048\n",
       "|    |    └─Mlp: 3-86                    8,393,728\n",
       "|    |    └─Identity: 3-87               --\n",
       "|    |    └─Identity: 3-88               --\n",
       "|    └─Block: 2-14                       --\n",
       "|    |    └─LayerNorm: 3-89              2,048\n",
       "|    |    └─Attention: 3-90              4,198,400\n",
       "|    |    └─Identity: 3-91               --\n",
       "|    |    └─Identity: 3-92               --\n",
       "|    |    └─LayerNorm: 3-93              2,048\n",
       "|    |    └─Mlp: 3-94                    8,393,728\n",
       "|    |    └─Identity: 3-95               --\n",
       "|    |    └─Identity: 3-96               --\n",
       "|    └─Block: 2-15                       --\n",
       "|    |    └─LayerNorm: 3-97              2,048\n",
       "|    |    └─Attention: 3-98              4,198,400\n",
       "|    |    └─Identity: 3-99               --\n",
       "|    |    └─Identity: 3-100              --\n",
       "|    |    └─LayerNorm: 3-101             2,048\n",
       "|    |    └─Mlp: 3-102                   8,393,728\n",
       "|    |    └─Identity: 3-103              --\n",
       "|    |    └─Identity: 3-104              --\n",
       "|    └─Block: 2-16                       --\n",
       "|    |    └─LayerNorm: 3-105             2,048\n",
       "|    |    └─Attention: 3-106             4,198,400\n",
       "|    |    └─Identity: 3-107              --\n",
       "|    |    └─Identity: 3-108              --\n",
       "|    |    └─LayerNorm: 3-109             2,048\n",
       "|    |    └─Mlp: 3-110                   8,393,728\n",
       "|    |    └─Identity: 3-111              --\n",
       "|    |    └─Identity: 3-112              --\n",
       "|    └─Block: 2-17                       --\n",
       "|    |    └─LayerNorm: 3-113             2,048\n",
       "|    |    └─Attention: 3-114             4,198,400\n",
       "|    |    └─Identity: 3-115              --\n",
       "|    |    └─Identity: 3-116              --\n",
       "|    |    └─LayerNorm: 3-117             2,048\n",
       "|    |    └─Mlp: 3-118                   8,393,728\n",
       "|    |    └─Identity: 3-119              --\n",
       "|    |    └─Identity: 3-120              --\n",
       "|    └─Block: 2-18                       --\n",
       "|    |    └─LayerNorm: 3-121             2,048\n",
       "|    |    └─Attention: 3-122             4,198,400\n",
       "|    |    └─Identity: 3-123              --\n",
       "|    |    └─Identity: 3-124              --\n",
       "|    |    └─LayerNorm: 3-125             2,048\n",
       "|    |    └─Mlp: 3-126                   8,393,728\n",
       "|    |    └─Identity: 3-127              --\n",
       "|    |    └─Identity: 3-128              --\n",
       "|    └─Block: 2-19                       --\n",
       "|    |    └─LayerNorm: 3-129             2,048\n",
       "|    |    └─Attention: 3-130             4,198,400\n",
       "|    |    └─Identity: 3-131              --\n",
       "|    |    └─Identity: 3-132              --\n",
       "|    |    └─LayerNorm: 3-133             2,048\n",
       "|    |    └─Mlp: 3-134                   8,393,728\n",
       "|    |    └─Identity: 3-135              --\n",
       "|    |    └─Identity: 3-136              --\n",
       "|    └─Block: 2-20                       --\n",
       "|    |    └─LayerNorm: 3-137             2,048\n",
       "|    |    └─Attention: 3-138             4,198,400\n",
       "|    |    └─Identity: 3-139              --\n",
       "|    |    └─Identity: 3-140              --\n",
       "|    |    └─LayerNorm: 3-141             2,048\n",
       "|    |    └─Mlp: 3-142                   8,393,728\n",
       "|    |    └─Identity: 3-143              --\n",
       "|    |    └─Identity: 3-144              --\n",
       "|    └─Block: 2-21                       --\n",
       "|    |    └─LayerNorm: 3-145             2,048\n",
       "|    |    └─Attention: 3-146             4,198,400\n",
       "|    |    └─Identity: 3-147              --\n",
       "|    |    └─Identity: 3-148              --\n",
       "|    |    └─LayerNorm: 3-149             2,048\n",
       "|    |    └─Mlp: 3-150                   8,393,728\n",
       "|    |    └─Identity: 3-151              --\n",
       "|    |    └─Identity: 3-152              --\n",
       "|    └─Block: 2-22                       --\n",
       "|    |    └─LayerNorm: 3-153             2,048\n",
       "|    |    └─Attention: 3-154             4,198,400\n",
       "|    |    └─Identity: 3-155              --\n",
       "|    |    └─Identity: 3-156              --\n",
       "|    |    └─LayerNorm: 3-157             2,048\n",
       "|    |    └─Mlp: 3-158                   8,393,728\n",
       "|    |    └─Identity: 3-159              --\n",
       "|    |    └─Identity: 3-160              --\n",
       "|    └─Block: 2-23                       --\n",
       "|    |    └─LayerNorm: 3-161             2,048\n",
       "|    |    └─Attention: 3-162             4,198,400\n",
       "|    |    └─Identity: 3-163              --\n",
       "|    |    └─Identity: 3-164              --\n",
       "|    |    └─LayerNorm: 3-165             2,048\n",
       "|    |    └─Mlp: 3-166                   8,393,728\n",
       "|    |    └─Identity: 3-167              --\n",
       "|    |    └─Identity: 3-168              --\n",
       "|    └─Block: 2-24                       --\n",
       "|    |    └─LayerNorm: 3-169             2,048\n",
       "|    |    └─Attention: 3-170             4,198,400\n",
       "|    |    └─Identity: 3-171              --\n",
       "|    |    └─Identity: 3-172              --\n",
       "|    |    └─LayerNorm: 3-173             2,048\n",
       "|    |    └─Mlp: 3-174                   8,393,728\n",
       "|    |    └─Identity: 3-175              --\n",
       "|    |    └─Identity: 3-176              --\n",
       "|    └─Block: 2-25                       --\n",
       "|    |    └─LayerNorm: 3-177             2,048\n",
       "|    |    └─Attention: 3-178             4,198,400\n",
       "|    |    └─Identity: 3-179              --\n",
       "|    |    └─Identity: 3-180              --\n",
       "|    |    └─LayerNorm: 3-181             2,048\n",
       "|    |    └─Mlp: 3-182                   8,393,728\n",
       "|    |    └─Identity: 3-183              --\n",
       "|    |    └─Identity: 3-184              --\n",
       "|    └─Block: 2-26                       --\n",
       "|    |    └─LayerNorm: 3-185             2,048\n",
       "|    |    └─Attention: 3-186             4,198,400\n",
       "|    |    └─Identity: 3-187              --\n",
       "|    |    └─Identity: 3-188              --\n",
       "|    |    └─LayerNorm: 3-189             2,048\n",
       "|    |    └─Mlp: 3-190                   8,393,728\n",
       "|    |    └─Identity: 3-191              --\n",
       "|    |    └─Identity: 3-192              --\n",
       "├─LayerNorm: 1-3                         2,048\n",
       "├─Linear: 1-4                            524,800\n",
       "├─Sequential: 1-5                        --\n",
       "|    └─Conv2d: 2-27                      1,792\n",
       "|    └─InstanceNorm2d: 2-28              --\n",
       "|    └─ReLU: 2-29                        --\n",
       "|    └─MaxPool2d: 2-30                   --\n",
       "├─Sequential: 1-6                        --\n",
       "|    └─Conv2d: 2-31                      73,856\n",
       "|    └─InstanceNorm2d: 2-32              --\n",
       "|    └─ReLU: 2-33                        --\n",
       "|    └─MaxPool2d: 2-34                   --\n",
       "├─Sequential: 1-7                        --\n",
       "|    └─Conv2d: 2-35                      295,168\n",
       "|    └─InstanceNorm2d: 2-36              --\n",
       "|    └─ReLU: 2-37                        --\n",
       "|    └─MaxPool2d: 2-38                   --\n",
       "├─Sequential: 1-8                        --\n",
       "|    └─Conv2d: 2-39                      1,180,160\n",
       "|    └─InstanceNorm2d: 2-40              --\n",
       "|    └─ReLU: 2-41                        --\n",
       "|    └─AdaptiveAvgPool2d: 2-42           --\n",
       "├─ModuleList: 1-9                        --\n",
       "|    └─CrossAttentionBlock: 2-43         --\n",
       "|    |    └─LayerNorm: 3-193             1,024\n",
       "|    |    └─Attention: 3-194             1,050,624\n",
       "|    |    └─Identity: 3-195              --\n",
       "|    |    └─LayerNorm: 3-196             1,024\n",
       "|    |    └─CrossAttention: 3-197        1,050,624\n",
       "|    |    └─Identity: 3-198              --\n",
       "|    |    └─LayerNorm: 3-199             1,024\n",
       "|    |    └─Mlp: 3-200                   2,099,712\n",
       "|    |    └─Identity: 3-201              --\n",
       "|    └─CrossAttentionBlock: 2-44         --\n",
       "|    |    └─LayerNorm: 3-202             1,024\n",
       "|    |    └─Attention: 3-203             1,050,624\n",
       "|    |    └─Identity: 3-204              --\n",
       "|    |    └─LayerNorm: 3-205             1,024\n",
       "|    |    └─CrossAttention: 3-206        1,050,624\n",
       "|    |    └─Identity: 3-207              --\n",
       "|    |    └─LayerNorm: 3-208             1,024\n",
       "|    |    └─Mlp: 3-209                   2,099,712\n",
       "|    |    └─Identity: 3-210              --\n",
       "├─LayerNorm: 1-10                        1,024\n",
       "├─Sequential: 1-11                       --\n",
       "|    └─Conv2d: 2-45                      1,179,904\n",
       "|    └─GroupNorm: 2-46                   512\n",
       "|    └─ReLU: 2-47                        --\n",
       "├─Sequential: 1-12                       --\n",
       "|    └─Conv2d: 2-48                      590,080\n",
       "|    └─GroupNorm: 2-49                   512\n",
       "|    └─ReLU: 2-50                        --\n",
       "├─Sequential: 1-13                       --\n",
       "|    └─Conv2d: 2-51                      590,080\n",
       "|    └─GroupNorm: 2-52                   512\n",
       "|    └─ReLU: 2-53                        --\n",
       "├─Sequential: 1-14                       --\n",
       "|    └─Conv2d: 2-54                      590,080\n",
       "|    └─GroupNorm: 2-55                   512\n",
       "|    └─ReLU: 2-56                        --\n",
       "|    └─Conv2d: 2-57                      257\n",
       "=================================================================\n",
       "Total params: 316,536,193\n",
       "Trainable params: 316,536,193\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from models_mae_cross import SupervisedMAE\n",
    "from torchsummary import summary\n",
    "\n",
    "model = SupervisedMAE()\n",
    "summary(model, input_size=(3, 384, 384))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
